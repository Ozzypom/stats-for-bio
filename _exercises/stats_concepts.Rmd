---
title: "Statistical Concepts"
output:
  html_document:
    css: ../extras.css
    theme: cerulean
    highlight: tango
---

You should work through the exercises step-by-step, following the instructions carefully. At various points we will interrupt the flow of instructions with a question. Make a note of your answers so that you can complete the MOLE quiz for this week.

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
```

## What kind of variable is it?

The following table gives a number of measurements taken in the course of a study of a woodland ecosystem. What type of variable results from the measurements taken in each case?

```{r, echo = FALSE}
table_data <- read.csv(file = "../tables_csv/variable_types.csv")
knitr::kable(
  table_data, booktabs = TRUE,
  caption = 'Examples of different kinds of variable.'
)
```

There are no answers to this question on MOLE. If you're not 100% sure what the right answer is in any of these examples, ask a TA or instructor for help.

## How does sample size influence the standard error?

The Soay coat example demonstrates that the amount of sampling variation associated with an estimate declines with sample size. The bigger our sample, the more precise the estimate of dark coat frequency, i.e. the smaller its standard error. The question we want to address now is: What form does the relationship between sample size and standard error take?

The snippet of R code below defines a function that will allow you to explore how the size of a sample influence the standard error of a frequency estimate. It expresses the frequency as a percentage:
```{r, eval = TRUE}
sample_sheep <- function(samp_sizes, prob) {
  sapply(samp_sizes, function (size) {
    raw_samples <- rbinom(n = 10000, size = size, prob = prob)
    sd(100 * raw_samples / size)
  })
}
```

To use this function you need to copy it into your script, highlight the whole thing, and then send it all to the console using Ctrl+Enter. This defines a custom R function called `sample_sheep` that will generate a set of standard errors associated with different sized samples.

You are not expected to understand how this works! You just need to be able to use the function:
```{r}
sample_sheep(samp_sizes = c(10, 20, 40, 100), prob = 0.4)
```
The first argument, `samp_sizes = c(10, 20, 40, 100)`, defines the set of sample sizes you want to use. This example will generate the standard error for samples of 10, 20, 40 and 100 individuals. The second argument, `prob = 0.4`, is the frequency of dark coat individuals in the population (expressed as a probability). The function returns a vector of numbers that are the **standard errors** of the frequency estimate at each sample size.

The simplest way to explore the relationship between sample size and standard error is to plot it. We need to collect together the inputs and outputs of these simulations into a data frame to do this:
```{r}
sim_data <- data.frame(sample_size = c(10, 20, 40, 100))
sim_data <- mutate(sim_data, se = sample_sheep(sample_size, prob = 0.4))
sim_data
```

Vary the sample size from around 20 to 500 indivduals, assuming that the dark coat frequency is fixed at 0.4 (`prob = 0.4`). You only need to vary the values assigned to `sample.size` to do this. The exact set of numbers you use does not matter too much. Just don't make the upper sample size much bigger than about 1000. 

```{block, type='do-something'}
**Question**

Does the standard error halve when you double the sample size, or is the relationship more complicated? If you think the relationship is more complicated, what form does it take?
```

HINT: Plot the standard error ('y' axis) against the sample size ('x' axis). Make several plots, trying out a range of different transformations of the sample size until you end up with a roughly linear relationship (i.e. a straight line).

